{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dmartins\\anaconda3\\envs\\mmSNN\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import snntorch as snn\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from snntorch import utils\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from snntorch import spikegen\n",
    "import numpy as np\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iteration 0, Train loss = 216.13 Test loss = 205.66 \n",
      "\n",
      "Test set accuracy for a single minibatch: 10.74%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 50, Train loss = 99.54 Test loss = 92.76 \n",
      "\n",
      "Test set accuracy for a single minibatch: 87.11%\n",
      "\n",
      "\n",
      "Epoch 0, Iteration 100, Train loss = 61.79 Test loss = 53.06 \n",
      "\n",
      "Test set accuracy for a single minibatch: 94.14%\n",
      "\n",
      "\n",
      "Epoch 1, Iteration 33, Train loss = 37.58 Test loss = 39.09 \n",
      "\n",
      "Test set accuracy for a single minibatch: 96.29%\n",
      "\n",
      "\n",
      "Epoch 1, Iteration 83, Train loss = 25.93 Test loss = 31.40 \n",
      "\n",
      "Test set accuracy for a single minibatch: 96.48%\n",
      "\n",
      "\n",
      "Epoch 2, Iteration 16, Train loss = 17.39 Test loss = 18.95 \n",
      "\n",
      "Test set accuracy for a single minibatch: 98.44%\n",
      "\n",
      "\n",
      "Epoch 2, Iteration 66, Train loss = 19.54 Test loss = 20.22 \n",
      "\n",
      "Test set accuracy for a single minibatch: 97.66%\n",
      "\n",
      "\n",
      "Epoch 2, Iteration 116, Train loss = 15.35 Test loss = 17.99 \n",
      "\n",
      "Test set accuracy for a single minibatch: 98.05%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataloader arguments\n",
    "batch_size = 128*4\n",
    "data_path='/tmp/data/mnist'\n",
    "\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "# Network Architecture\n",
    "num_inputs = 28*28\n",
    "\n",
    "num_epochs = 3\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "\n",
    "# Define Network\n",
    "class VisNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden=1000, num_last_hidden=20, num_output=10,\n",
    "                 num_steps=81, beta=0.95):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_hidden)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "        self.fc3 = nn.Linear(num_hidden, num_last_hidden)\n",
    "        self.lif3 = snn.Leaky(beta=beta)\n",
    "        self.fc4 = nn.Linear(num_last_hidden, num_output)\n",
    "        self.lif4 = snn.Leaky(beta=beta)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "        mem4 = self.lif4.init_leaky()\n",
    "\n",
    "        # Record the output layer\n",
    "        output_spike_record = []\n",
    "        output_memV_record = []\n",
    "\n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "            cur4 = self.fc4(spk3)\n",
    "            spk4, mem4 = self.lif4(cur4, mem4)\n",
    "\n",
    "            output_spike_record.append(spk4)\n",
    "            output_memV_record.append(mem4)\n",
    "\n",
    "        self.mem1 = mem1\n",
    "        self.mem2 = mem2\n",
    "        self.mem3 = mem3\n",
    "\n",
    "        return  torch.stack(output_spike_record, dim=0), torch.stack(output_memV_record, dim=0)\n",
    "    \n",
    "    def fwd_frozen(self, x):\n",
    "\n",
    "        mem1 = self.mem1\n",
    "        mem2 = self.mem2\n",
    "        mem3 = self.mem3\n",
    "\n",
    "        last_hidden_spike_record = []\n",
    "        last_hidden_output_memV_record = []\n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "            last_hidden_spike_record.append(spk3)\n",
    "            last_hidden_output_memV_record.append(mem3)\n",
    "\n",
    "        return torch.stack(last_hidden_spike_record, dim=0), torch.stack(last_hidden_output_memV_record, dim=0)\n",
    "\n",
    "\n",
    "# Load the network onto CUDA if available\n",
    "net = VisNet(num_inputs=28*28).to(device)\n",
    "\n",
    "def print_batch_accuracy(data, targets, train=False):\n",
    "    output, _ = net(data.view(batch_size, -1))\n",
    "    _, idx = output.sum(dim=0).max(1)\n",
    "    acc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "\n",
    "    # if train:\n",
    "    #     print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "    # else:\n",
    "    print(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "\n",
    "def train_printer():\n",
    "    print(f\"Epoch {epoch}, Iteration {iter_counter}, Train loss = {loss_hist[counter]:.2f} Test loss = {test_loss_hist[counter]:.2f} \\n\")\n",
    "    print_batch_accuracy(data, targets)\n",
    "    print('\\n')\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "    iter_counter = 0\n",
    "    train_batch = iter(train_loader)\n",
    "\n",
    "    # Minibatch training loop\n",
    "    for data, targets in train_batch:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        net.train()\n",
    "        spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "\n",
    "        # initialize the loss & sum over time\n",
    "        loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "        for step in range(net.num_steps):\n",
    "            loss_val += loss(mem_rec[step], targets)\n",
    "\n",
    "        # Gradient calculation + weight update\n",
    "        optimizer.zero_grad()\n",
    "        loss_val.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history for future plotting\n",
    "        loss_hist.append(loss_val.item())\n",
    "\n",
    "        # Test set\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            test_data, test_targets = next(iter(test_loader))\n",
    "            test_data = test_data.to(device)\n",
    "            test_targets = test_targets.to(device)\n",
    "\n",
    "            # Test set forward pass\n",
    "            test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
    "\n",
    "            # Test set loss\n",
    "            test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
    "            for step in range(net.num_steps):\n",
    "                test_loss += loss(test_mem[step], test_targets)\n",
    "            test_loss_hist.append(test_loss.item())\n",
    "\n",
    "            # Print train/test loss/accuracy\n",
    "            if counter % 50 == 0:\n",
    "                train_printer()\n",
    "            counter += 1\n",
    "            iter_counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 784])\n",
      "torch.Size([1000])\n",
      "torch.Size([1000, 1000])\n",
      "torch.Size([1000])\n",
      "torch.Size([20, 1000])\n",
      "torch.Size([20])\n",
      "torch.Size([10, 20])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for params in net.parameters():\n",
    "    print(params.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_spk, all_test_mem = [], []\n",
    "all_test_data, all_test_targets = [], []\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    for test_data, test_targets in iter(test_loader):\n",
    "    \n",
    "        # test_data, test_targets = next(iter(test_loader))\n",
    "        test_data = test_data.to(device)\n",
    "        test_targets = test_targets.to(device)\n",
    "\n",
    "        # Test set forward pass\n",
    "        test_spk, test_mem = net.fwd_frozen(test_data.view(batch_size, -1))\n",
    "        all_test_spk.append(test_spk)\n",
    "        all_test_mem.append(test_mem)\n",
    "        all_test_data.append(test_data)\n",
    "        all_test_targets.append(test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_inputs = np.zeros([\n",
    "    len(all_test_spk)*np.size(all_test_spk[0],1),\n",
    "    28,\n",
    "    28\n",
    "])\n",
    "visual_labels = np.zeros(len(all_test_spk)*np.size(all_test_spk[0],1))\n",
    "for i in range(len(all_test_spk)):\n",
    "    visinput = all_test_data[i]\n",
    "    vistarg = all_test_targets[i]\n",
    "    ind_start = i*np.size(all_test_spk[0],1)\n",
    "    ind_end = i*np.size(all_test_spk[0],1) + np.size(all_test_spk[0],1)\n",
    "    visual_inputs[ind_start:ind_end, :, :] = np.squeeze(visinput).numpy()\n",
    "    visual_labels[ind_start:ind_end] = vistarg.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_outputs = np.zeros([\n",
    "    2,\n",
    "    81,\n",
    "    len(all_test_spk)*np.size(all_test_spk[0],1),\n",
    "    20\n",
    "])\n",
    "for i in range(len(all_test_spk)):\n",
    "    spk = all_test_spk[i]\n",
    "    mem = all_test_mem[i]\n",
    "    ind_start = i*np.size(all_test_spk[0],1)\n",
    "    ind_end = i*np.size(all_test_spk[0],1) + np.size(all_test_spk[0],1)\n",
    "    spike_outputs[0, :, ind_start:ind_end, :] = spk.numpy()\n",
    "    spike_outputs[1, :, ind_start:ind_end, :] = mem.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9728, 28, 28) (9728,) (2, 81, 9728, 20)\n"
     ]
    }
   ],
   "source": [
    "print(visual_inputs.shape, visual_labels.shape, spike_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\n",
    "    'visual_outputs.npz',\n",
    "    inputs=visual_inputs,\n",
    "    labels=visual_labels,\n",
    "    outputs=spike_outputs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmSNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
