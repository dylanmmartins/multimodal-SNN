{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependences \n",
    "import snntorch as snn\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from snntorch import utils\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from snntorch import spikegen\n",
    "import numpy as np\n",
    "\n",
    "dtype = torch.float\n",
    "torch.set_default_dtype(dtype)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\") #uses the GPU when is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_stim = np.load('auditory_stimuli.npz') #load the mpz file as the auditory stimulus the keys are x_train, y_train, x_test, y_test\n",
    "X_train_raw = audio_stim['X_train'] #size of (2100, 129, 81) number of samples x frequency x time \n",
    "X_test_raw = audio_stim['X_test'] #size of (900, 129, 81)\n",
    "y_train = audio_stim['y_train'] #size (2100 by 1)\n",
    "y_test = audio_stim['y_test'] #size of 900 by 1\n",
    "\n",
    "\n",
    "\n",
    "#print(X_train_raw.shape, y_train.shape, X_test_raw.shape, y_test.shape) #(2100, 129, 81) (2100,) (900, 129, 81) (900,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X_train = np.zeros([\n",
    "    np.size(X_train_raw,0),\n",
    "    np.size(X_train_raw,1) * np.size(X_train_raw,2)\n",
    "]) #this is a 2100 by 10449 array of zeros\n",
    "\n",
    "X_test = np.zeros([\n",
    "    np.size(X_test_raw,0),\n",
    "    np.size(X_test_raw,1) * np.size(X_test_raw,2)\n",
    "]) #this is 900 by 10449\n",
    "\n",
    "for i in range(np.size(X_train_raw,0)): #from 0 to 2100\n",
    "    X_train[i,:] = np.ravel(X_train_raw[i,:,:]) #x_train(i,:) is being set to the flatten version of 1x129x81 of the raw data\n",
    "\n",
    "for i in range(np.size(X_test_raw,0)):\n",
    "    X_test[i,:] = np.ravel(X_test_raw[i,:,:]) #same flatten procedure as the for loop above\n",
    "    \n",
    "    #this cell ends with x_train and x_test, being arrays containing the flatten version of the raw data in each cell\n",
    "'''\n",
    "X_train = X_train_raw\n",
    "X_test = X_test_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader arguments\n",
    "batch_size = 128*4 #set the batch size to 512 \n",
    "\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "training_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_train.astype(np.float32)),\n",
    "    torch.from_numpy(y_train.astype(np.float32))\n",
    ") # creates a tensor of float32 containing the training data and label \n",
    "\n",
    "testing_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_test.astype(np.float32)),\n",
    "    torch.from_numpy(y_test.astype(np.float32))\n",
    ") #creates a dataset for testing containihng data and label in a float32 format\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True, drop_last=True) \n",
    "test_loader = DataLoader(testing_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100 #control the number of epochs in the training\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "\n",
    "# Define Network\n",
    "class AudNet(nn.Module):\n",
    "\t#no inputs for init,, 1000 hidden layers, 10 outputs, 81 steps, frequency 129, \n",
    "\tdef __init__(self,\n",
    "\t\tnum_inputs=None, num_hidden=1000, num_last_hidden=20, num_output=10,\n",
    "\t\tbeta=0.95, num_steps=81, num_freq=129):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tif num_inputs is None:\n",
    "\t\t\tnum_inputs = num_steps * num_freq #number of inputs will be set by multiplying steps by frequency, or 129*81 if number of inputs is 0\n",
    "\n",
    "\t\tself.num_steps = num_steps\n",
    "\t\tself.num_freq = num_freq\n",
    "\n",
    "\t\t# Initialize layers \n",
    "\t\tself.fc1 = nn.Linear(num_inputs, num_hidden) #first layer with num_inputs inputs and 1000 hidden \n",
    "\t\tself.lif1 = snn.Leaky(beta=beta) #first layer of spiking neuro network, controlled by beta \n",
    "\t\tself.fc2 = nn.Linear(num_hidden, num_hidden)\n",
    "\t\tself.lif2 = snn.Leaky(beta=beta)\n",
    "\t\tself.fc3 = nn.Linear(num_hidden, num_last_hidden)\n",
    "\t\tself.lif3 = snn.Leaky(beta=beta)\n",
    "\t\tself.fc4 = nn.Linear(num_last_hidden, num_output)\n",
    "\t\tself.lif4 = snn.Leaky(beta=beta)\n",
    "\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\n",
    "\t\tx.to(torch.float32) #converts x to a float32 \n",
    "\t\tn = x.shape[0] #sets n to the first dimension of x\n",
    "\n",
    "\t\t# Initialize hidden states at t=0\n",
    "\t\tmem1 = self.lif1.init_leaky()\n",
    "\t\tmem2 = self.lif2.init_leaky()\n",
    "\t\tmem3 = self.lif3.init_leaky()\n",
    "\t\tmem4 = self.lif3.init_leaky() #why lif3 and not lif4\n",
    "\n",
    "\t\toutput_spike_record = []\n",
    "\t\toutput_memV_record = []\n",
    "\n",
    "\t\tfor step in range(self.num_steps):\n",
    "\n",
    "\t\t\tx_ = x.reshape(n, self.num_freq, self.num_steps)[:,:,step] #sets x_ to be the same dimensions as the original data \n",
    "\n",
    "\t\t\tcur1 = self.fc1(x_) \n",
    "\t\t\tspk1, mem1 = self.lif1(cur1, mem1)\n",
    "\n",
    "\t\t\tcur2 = self.fc2(spk1)\n",
    "\t\t\tspk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "\t\t\tcur3 = self.fc3(spk2)\n",
    "\t\t\tspk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "\t\t\tcur4 = self.fc4(spk3)\n",
    "\t\t\tspk4, mem4 = self.lif4(cur4, mem4)\n",
    "\n",
    "\t\t\toutput_spike_record.append(spk4)\n",
    "\t\t\toutput_memV_record.append(mem4)\n",
    "\n",
    "\t\tself.mem1 = mem1\n",
    "\t\tself.mem2 = mem2\n",
    "\t\tself.mem3 = mem3\n",
    "\n",
    "\t\treturn  torch.stack(output_spike_record, dim=0), torch.stack(output_memV_record, dim=0)\n",
    "\n",
    "\tdef fwd_frozen(self, x):\n",
    "\n",
    "\t\tmem1 = self.mem1\n",
    "\t\tmem2 = self.mem2\n",
    "\t\tmem3 = self.mem3\n",
    "\n",
    "\t\tlast_hidden_spike_record = []\n",
    "\t\tlast_hidden_output_memV_record = []\n",
    "\n",
    "\t\tfor step in range(self.num_steps):\n",
    "\n",
    "\t\t\tx_ = x.reshape(self.num_freq, self.num_steps)[:,step]\n",
    "\n",
    "\t\t\tcur1 = self.fc1(x_)\n",
    "\t\t\tspk1, mem1 = self.lif1(cur1, mem1)\n",
    "\n",
    "\t\t\tcur2 = self.fc2(spk1)\n",
    "\t\t\tspk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "\t\t\tcur3 = self.fc3(spk2)\n",
    "\t\t\tspk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "\t\t\tlast_hidden_spike_record.append(spk3)\n",
    "\t\t\tlast_hidden_output_memV_record.append(mem3)\n",
    "\n",
    "\t\treturn torch.stack(last_hidden_spike_record, dim=0), torch.stack(last_hidden_output_memV_record, dim=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sz = np.size(X_train, 1) #sets input size to  the second dimension of x_train. For us 129\n",
    "# Load the network onto CUDA if available\n",
    "net = AudNet(num_inputs=input_sz).to(device) #loads the data using 129 as the number of inputs\n",
    "#print(\"network loaded\")\n",
    "\n",
    "# pass data into the network, sum the spikes over time\n",
    "# and compare the neuron with the highest number of spikes\n",
    "# with the target\n",
    "\n",
    "def print_batch_accuracy(data, targets, train=False):\n",
    "\toutput, _ = net(data)\n",
    "\t_, idx = output[-1].max(1)\n",
    "\tacc = np.mean((targets == idx).detach().cpu().numpy())\n",
    "\n",
    "\t# if train:\n",
    "\t#     print(f\"Train set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "\t# else:\n",
    "\tprint(f\"Test set accuracy for a single minibatch: {acc*100:.2f}%\")\n",
    "\n",
    "#print(\"function print_batch_accuracy created\")\n",
    "\n",
    "def train_printer():\n",
    "\tprint(f\"Epoch {epoch}, Iteration {iter_counter}, Train loss = {loss_hist[counter]:.2f} Test loss = {test_loss_hist[counter]:.2f} \\n\")\n",
    "\tprint_batch_accuracy(data, targets)\n",
    "\tprint('\\n')\n",
    "\n",
    "#print(\"train_printer created\")\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "\n",
    "# Outer training loop\n",
    "for epoch in range(num_epochs):\n",
    "\titer_counter = 0\n",
    "\ttrain_batch = iter(train_loader)\n",
    "\t#print(epoch)\n",
    "\t# Minibatch training loop\n",
    "\tfor data, targets in train_batch:\n",
    "\t\tdata = data.to(device)\n",
    "\t\ttargets = targets.to(device)\n",
    "\t\t#print(data)\n",
    "\t\t#print(targets)\n",
    "\t\t# forward pass\n",
    "\t\tnet.train()\n",
    "\t\t#print(\"network trained\")\n",
    "\t\t#spk_rec, mem_rec = net(data.view(batch_size, -1)) #batch x freq * freq*num_steps x hidden \n",
    "\t\tspk_rec, mem_rec = net(data)\n",
    "\t\t#print(\"spk_rec, mem_rec\")\n",
    "\t\t# initialize the loss & sum over time\n",
    "\t\t# loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "\t\tloss_val = loss(mem_rec[-1], targets.type(torch.long))\n",
    "\n",
    "\t\t# Gradient calculation + weight update\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss_val.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\t# Store loss history for future plotting\n",
    "\t\tloss_hist.append(loss_val.item())\n",
    "\n",
    "\t\t# Test set\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tnet.eval()\n",
    "\t\t\ttest_data, test_targets = next(iter(test_loader))\n",
    "\t\t\ttest_data = test_data.to(device)\n",
    "\t\t\ttest_targets = test_targets.to(device)\n",
    "\n",
    "\t\t\t# Test set forward pass\n",
    "\t\t\t#test_spk, test_mem = net(test_data.view(batch_size, -1))\n",
    "\t\t\ttest_spk, test_mem  = net(test_data)\n",
    "\n",
    "\t\t\t# Test set loss\n",
    "\t\t\t# test_loss = torch.zeros((1), dtype=dtype, device=device)\n",
    "\t\t\t# for step in range(num_steps):\n",
    "\t\t\ttest_loss = loss(test_mem[-1], test_targets.type(torch.long))\n",
    "\t\t\ttest_loss_hist.append(test_loss.item())\n",
    "\n",
    "\t\t\t# Print train/test loss/accuracy\n",
    "\t\t\tif counter % 10 == 0:\n",
    "\t\t\t\ttrain_printer()\n",
    "\t\t\tcounter += 1\n",
    "\t \n",
    "\t\titer_counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_printer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.view(batch_size, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#129*81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, _ = net(data.view(batch_size, -1))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, idx = output.max(1)\n",
    "# acc = np.mean((targets == idx).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mem.shape, test_targets.type(torch.long).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, s = spectrogram(X_train[2,:], fs=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train[2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(s)\n",
    "plt.xlim([0,20])\n",
    "plt.ylim([0,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train[1,:], 'k')\n",
    "plt.xlim([0,2400])\n",
    "plt.xlabel('samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_rec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spk_rec, mem_rec = net(data.view(batch_size, -1))\n",
    "spk_rec.shape, mem_rec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in net.parameters():\n",
    "    print(param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NpzFile 'auditory_stim_outputs.npz' with keys: inputs, labels, outputs\n"
     ]
    }
   ],
   "source": [
    "#print(output.type)\n",
    "\n",
    "\n",
    "np.savez(\n",
    "\n",
    "    'auditory_stim_outputs.npz',\n",
    "    inputs = np.concatenate((X_train_raw, X_test_raw), axis=0),\n",
    "    labels=np.concatenate((y_train, y_test), axis=0, out=None),\n",
    "    outputs =output.detach().cpu().numpy()\n",
    "     )\n",
    "\n",
    "\n",
    "\n",
    "#print(auditory_stim_outputs)\n",
    "torch.save(net.state_dict(), \"aud_v1.pt\")\n",
    "file = np.load(\"auditory_stim_outputs.npz\")\n",
    "print(file)\n",
    "\n",
    "#print(auditory_stim_outputs.npz)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmSNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
