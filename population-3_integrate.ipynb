{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34fb7ad2-8ac8-44d0-a8a1-00c6d0475d6f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89b7617-2a38-4726-b98e-f2dfb08e425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import snntorch as snn\n",
    "import snntorch.functional as SF\n",
    "from snntorch import spikeplot as splt\n",
    "import snntorch.spikegen as spikegen\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cca4ba-064b-4323-94e2-546e08ae9860",
   "metadata": {},
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd53326-d020-4e30-be54-03f014a90b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalIntegration(nn.Module):\n",
    "    \"\"\"\n",
    "    Combine the outputs from the visual and auditory encoders into a single multimodal representation.\n",
    "    \"\"\"\n",
    "    def __init__(self, multimodal_size, output_size, beta):\n",
    "        super(MultimodalIntegration, self).__init__()\n",
    "        self.fc = nn.Linear(multimodal_size, output_size)\n",
    "        self.lif = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cur = self.fc(x)\n",
    "        spk, mem = self.lif(cur)\n",
    "        return spk, mem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dadabcd-5ce3-427a-bea2-d9e33c9aba05",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cc23d0-63b2-46b2-864b-1100398bb40a",
   "metadata": {},
   "source": [
    "## Create models\n",
    "\n",
    "We have the output from Dylan's visual model. Just use random for audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b69467-1a34-4070-8d27-700c2249e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual_size = 5  # Length of visual input vector\n",
    "# audio_size = 3   # Length of audio input vector\n",
    "hidden_size = 50  # Number of neurons in the hidden layers\n",
    "beta = 0.9  # Decay rate of the LIF neuron\n",
    "multimodal_size = hidden_size * 2  # Combined size for multimodal layer\n",
    "\n",
    "# Output size for the final multimodal representation\n",
    "output_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aa0f55-8d03-4228-ac81-fbf5976cde17",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MultimodalIntegration(multimodal_size, output_size, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fb1026-2b22-4c15-8627-00046dc60917",
   "metadata": {},
   "source": [
    "## Load visual representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de54f268-8bae-4a5e-9281-51022459d2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 81, 9728, 20)\n"
     ]
    }
   ],
   "source": [
    "npzfile = np.load(\"visual_outputs.npz\")\n",
    "#print(npzfile)\n",
    "\"\"\"\n",
    "'inputs'\n",
    "\n",
    "'labels'\n",
    "    dim0: training examples; 0-9\n",
    "\n",
    "'outputs'\n",
    "    dim0: spks, mems\n",
    "    dim1: time stpes\n",
    "    dim2: training examples\n",
    "    dim3: output neurons\n",
    "\"\"\"\n",
    "vis_labels = npzfile[\"labels\"] # (9984,)\n",
    "vis_outputs = npzfile[\"outputs\"] # (2, 25, 9984, 50)\n",
    "print(np.shape(vis_outputs))\n",
    "\n",
    "\n",
    "\n",
    "vis_spks = np.transpose(vis_outputs[0,:,:,:], (1, 0, 2)) # (9984, 25, 50)\n",
    "vis_mems = np.transpose(vis_outputs[1,:,:,:], (1, 0, 2)) # (9984, 25, 50)\n",
    "#print(vis_spks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72473512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 512, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npzfile_audio  = np.load(\"auditory_stim_outputs.npz\")\n",
    "#print(npzfile)\n",
    "\"\"\"\n",
    "'inputs'\n",
    "\n",
    "'labels'\n",
    "    dim0: training examples; 0-9\n",
    "\n",
    "'outputs'\n",
    "    dim0: spks, mems\n",
    "    dim1: time stpes\n",
    "    dim2: training examples\n",
    "    dim3: output neurons\n",
    "\"\"\"\n",
    "aud_labels = npzfile_audio[\"labels\"] # (9984,)\n",
    "aud_outputs = npzfile_audio[\"outputs\"] # (2, 25, 9984, 50)\n",
    "#np.shape(aud_outputs)\n",
    "\n",
    "#aud_spks = np.transpose(aud_outputs[0,:,:,:], (1, 0, 2)) # (9984, 25, 50)\n",
    "#aud_mems = np.transpose(aud_outputs[1,:,:,:], (1, 0, 2)) # (9984, 25, 50)\n",
    "# print(vis_spks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd1ec04-c98c-4da0-85cf-6973bdcb1c62",
   "metadata": {},
   "source": [
    "## Run stuff through them (no training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148dbaf9-f110-433a-a47e-adf1f456ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create example input data\n",
    "# visual_input = torch.rand((visual_size,))\n",
    "# audio_input = torch.rand((audio_size,))\n",
    "\n",
    "# # Generate spike trains from input data\n",
    "# visual_spike_train = spikegen.rate(visual_input, num_steps=100)\n",
    "# audio_spike_train = spikegen.rate(audio_input, num_steps=100)\n",
    "\n",
    "# # Forward pass through the encoding layers\n",
    "# visual_spk, mem = visual_encoder(visual_spike_train.float())\n",
    "# audio_spk, mem = audio_encoder(audio_spike_train.float())\n",
    "# print(audio_spk.shape)\n",
    "\n",
    "#print(torch.load(\"aud_v1.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd3ebe-3bba-49ed-ae25-628ba9bcf338",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_rep = torch.from_numpy(vis_spks[0]).float()\n",
    "#aud_output = torch.from_numpy(vis_spks[0]).float()\n",
    "#np.load(\"\")\n",
    "print(vis_rep.dtype)\n",
    "\n",
    "\n",
    "# Concatenate the encoded spikes\n",
    "combined_spikes = torch.cat((vis_rep, aud_output), dim=1)\n",
    "print(combined_spikes.shape)\n",
    "\n",
    "# Forward pass through the integration layer\n",
    "multimodal_output, mem = net(combined_spikes)\n",
    "print(multimodal_output.shape)\n",
    "\n",
    "# print(\"Multimodal output spike vector:\", multimodal_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0dbb5b-038e-45ed-8a13-7e0a78d83fe6",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78870c1d-8b9f-4a02-a13a-4f3f819176dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "# Training data is output from visual and audio encoders\n",
    "vis_spikes = torch.from_numpy(vis_spks).float()\n",
    "aud_spikes = torch.from_numpy(vis_spks).float() # audio doesn't work so just copy vision\n",
    "targets = torch.from_numpy(vis_labels).long() # (9984,)\n",
    "\n",
    "# one-hot encode targets\n",
    "one_hot_targets = torch.zeros(9984, 10)\n",
    "for i in range(len(targets)):\n",
    "    one_hot_targets[i, targets[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336a8c19-6a3c-4276-881a-fba50e3532a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "num_samples = targets.shape[0]\n",
    "num_steps = 25  # Number of timesteps for the spike train\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for i in range(num_samples):\n",
    "        target = one_hot_targets[i]\n",
    "\n",
    "        # Concatenate output spike trains from single sensory encoders\n",
    "        spike_train = torch.cat((vis_spikes[i,:,:], aud_spikes[i,:,:]), dim=1)\n",
    "\n",
    "        # Initialize hidden states\n",
    "        mem1 = net.lif.init_leaky()\n",
    "\n",
    "        # Accumulate loss over timesteps\n",
    "        total_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        for t in range(num_steps):\n",
    "            # Forward pass through the network for each timestep\n",
    "            cur1 = net.fc(spike_train[t])\n",
    "            spk1, mem1 = net.lif(cur1, mem1)\n",
    "\n",
    "            # Compute loss for this timestep\n",
    "            loss = loss_fn(spk1, target)\n",
    "            total_loss += loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += total_loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / num_samples\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "        \n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de74e0b-6720-4d90-bb79-6f4525bb482b",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf266f5-5ac2-42ae-8bbe-ac73e0cd9eaa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Plotting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578088aa-56ff-489f-a775-5ace8c552f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_and_plot_model_inference(visual_spikes, audio_spikes, targets, ex=0, hidden_size=hidden_size, audio_type=\"random\"):\n",
    "    \"\"\"\n",
    "        audio_type: \"random\", \"visual_copy\"\n",
    "        \n",
    "    \"\"\"\n",
    "    visual_spk = vis_spikes[ex,:,:]\n",
    "\n",
    "    if audio_type == \"visual_copy\":\n",
    "        audio_spk = visual_spk\n",
    "    elif audio_type == \"random\":\n",
    "        audio_input = torch.rand((hidden_size,))\n",
    "        audio_spk = spikegen.rate(audio_input, num_steps=25)\n",
    "    else:\n",
    "        print(\"Not an option...\")\n",
    "        return\n",
    "        \n",
    "    target = targets[ex]\n",
    "\n",
    "    # Concatenate the encoded spikes\n",
    "    combined_spikes = torch.cat((visual_spk, audio_spk), dim=1)\n",
    "\n",
    "    # Forward pass through the integration layer\n",
    "    multimodal_output, mem = net(combined_spikes)\n",
    "\n",
    "    print(f\"Vision Input: shape={visual_spk.shape}, spikes={visual_spk.sum()}\")\n",
    "    print(f\"Audio: shape={audio_spk.shape}, spikes={audio_spk.sum()}\")\n",
    "    print(f\"Multimodal: shape={multimodal_output.shape}, spikes={multimodal_output.sum()}\")\n",
    "    print(f\"Ground truth: {target}\")\n",
    "\n",
    "    fig, ax = plt.subplots(3, figsize=(8,7), sharex=True, \n",
    "                        gridspec_kw = {'height_ratios': [1, 1, 0.4]})\n",
    "    \n",
    "    # # Plot input spikes\n",
    "    # splt.raster(visual_spike_train, ax[0], s=5, c=\"black\")\n",
    "    # ax[0].set_ylabel(\"Vision Input\")\n",
    "    # ax[0].set_title(\"Multimodal Integration\")\n",
    "    \n",
    "    # Plot hidden layer spikes\n",
    "    splt.raster(visual_spk, ax[0], s=5, c=\"black\")\n",
    "    ax[0].set_ylabel(\"Vision Hidden\")\n",
    "    \n",
    "    # # Plot input spikes\n",
    "    # splt.raster(audio_spike_train, ax[2], s=5, c=\"black\")\n",
    "    # ax[2].set_ylabel(\"Audio Input\")\n",
    "    \n",
    "    # Plot hidden layer spikes\n",
    "    splt.raster(audio_spk, ax[1], s=5, c=\"black\")\n",
    "    ax[1].set_ylabel(\"Audio Hidden\")\n",
    "    \n",
    "    # Plot output spikes\n",
    "    splt.raster(multimodal_output.reshape(num_steps, -1), ax[2], c=\"black\", marker=\"|\")\n",
    "    ax[2].set_ylabel(\"Multimodal Output\")\n",
    "\n",
    "    ax[0].set_ylim((-2.45, 51.45))\n",
    "    ax[1].set_ylim((-2.45, 51.45))\n",
    "    ax[2].set_ylim([0,9])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    ## Can also plot mem; will show for each neuron\n",
    "    #plt.plot(memv.detach().numpy())\n",
    "    #plt.plot(mem.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20db8a0-8bde-479f-8167-84c0b8866cf1",
   "metadata": {},
   "source": [
    "### Visual and audio the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ed4350-bca4-4a0f-a56f-121a8a40b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_and_plot_model_inference(vis_spikes, None, targets, ex=1, audio_type=\"visual_copy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769e0c9d-834b-4735-91dd-087fc679bc5d",
   "metadata": {},
   "source": [
    "### Vision good but audio random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70dcbb3-eb6d-4234-97e7-ba5e783eda68",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_and_plot_model_inference(vis_spikes, None, targets, ex=1, audio_type=\"random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf52419-3ac4-4412-b4da-2fec81bfbc74",
   "metadata": {},
   "source": [
    "# Questions and todo\n",
    "\n",
    "* include audio hidden output\n",
    "* how to convert multimodal output spike train into final answer -- probability??\n",
    "* write code for systematically evaluating model\n",
    "    * in no noise, 25% noise, 50% noise, 75% noise, 100% noise\n",
    "* train model end-to-end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
